---
title: "Generalized Linear Models (+)"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: bamlss.bib
nocite: '@bamlss:Umlauf+bamlss:2018'
vignette: >
  %\VignetteIndexEntry{Model Terms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{bamlss}
  %\VignetteKeywords{distributional regression, GLM, big data}
  %\VignettePackage{bamlss}
---

```{r preliminaries, echo=FALSE, message=FALSE}
library("bamlss")
set.seed(123)
```

## Intro

The _bamlss_ package is perfectly suitable for estimating (Bayesian) generalized linear 
models (GLM) and provides infrastructures for the estimation using very large data sets,
too.  Within the main model fitting function `bamlss()`, the possible `family` 
specifications for fitting GLMs are: 

* `"gaussian"` or `gaussian_bamlss()`,
* `"beta"` or `beta_bamlss()`,
* `"binomial"` or `binomial_bamlss()`,
* `"gamma"` or `gamma_bamlss()`,
* `"poisson"` or `poisson_bamlss()`.

In addition, there is a wrapper function for the family objects provided by the _gamlss_
package [@bamlss:Stasinopoulos+Rigby:2018], so in principle all _gamlss_ families can be used by
just passing them to the `family` argument in the `bamlss()` function (see also the
[count data regression example](#countreg)).

First, we illustrate how  to fit standard GLMs and how to do inference using the _bamlss_ 
framework. Aftwards, we show how to estimate GLMs using very large data set.

## Logit model

This example is taken from the _AER_ package
[@bamlss:Kleiber+Zeileis:2018; @bamlss:Kleiber+Zeileis:2008] and is about labor force 
participation (yes/no) of women in Switzerland 1981. The data can be loaded with
```{r}
data("SwissLabor", package = "AER")
print(head(SwissLabor))
```
The data frame contains of 872 observations on 7 variables, where some of them might have
a nonlinear influence on the response labor `participation`. Now, a standard Bayesian 
binomial logit model can be fitted with
```{r, echo=FALSE, message=FALSE, results='hide'}
if(!file.exists("figures/SwissLabor.rda")) {
  f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)
  set.seed(123)
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "figures/SwissLabor.rda")
} else {
  load("figures/SwissLabor.rda")
}
```
```{r, eval=FALSE}
library("bamlss")

## First, set the seed for reproducibly.
set.seed(123)

## Model formula.
f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)

## Estimate model.
b <- bamlss(f, family = "binomial", data = SwissLabor,
  n.iter = 12000, burnin = 2000, thin = 10)
```
Note, to capture nonlinearities, a quadratic term for variable `age` is added to the model. The model summary gives
```{r}
summary(b)
```
which suggests "significant" effects for all covariates, since there are no zeros within
the 95% credible intervals. Before we proceed, we usually do some convergence checks of
the MCMC chains using traceplots.
```{r, eval=FALSE}
plot(b, which = "samples")
```
```{r, fig.width = 9, fig.height = 5, fig.align = "center", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
bsp <- b
bsp$samples <- bsp$samples[, c("pi.p.(Intercept)", "pi.p.income")]
plot(bsp, which = "samples")
```
The plots indicate convergence of the MCMC chains, i.e., there is no visible trend in the
MCMC chains and very low autocorrelation suggest i.i.d. sampling from the posterior
distribution. Note that the function call would show all traceplots, however, for 
convenience we only show the plots for the intercept and variable `income`.

The estimated regression coefficients can also be extracted using the `coef()` method
```{r}
## Extract posterior mean.
coef(b, FUN = mean)

## Or use any other function on the samples.
coef(b, FUN = function(x) { quantile(x, prob = c(0.025, 0.975)) })
```
(there is also a `confint()` method). The naming convention of the regression coefficients 
might first seem a bit atypical, it is based on the idea that a _bamlss_ family/response 
distribution can have more than just one distributional parameter, as well as 
linear and/or nonlinear model terms. To explain, `pi` is the name of the distributional 
parameter in the `binomial_bamlss()` family and `p` stands for parametric terms, i.e.,
there would also be a `s` for smooth terms if they are part of the model.

Model predictions on the probability scale can be obtained by the predict method (see also
function `predict.bamlss()`).
```{r}
## Create some newdata for prediction, note that
## factors need to be fully specified (this will be changed soon).
nd <- data.frame(income = 11, age = 3.3,
  education = 12, youngkids = 1, oldkids = 1,
  foreign = factor(1, levels = 1:2, labels = c("no", "yes")))

## Predicted probabilities.
predict(b, newdata = nd, type = "parameter")
nd$foreign <- factor(2, levels = 1:2, labels = c("no", "yes"))
predict(b, newdata = nd, type = "parameter")
```
To visualize the effect of age on the probability we can do the following:
```{r, eval=FALSE}
## Predict effect of age including 95% credible intervals and plot.
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1,
  foreign = factor(1, levels = 1:2, labels = c("no", "yes")))

nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- factor(2, levels = 1:2, labels = c("no", "yes"))
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)

plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"))
```
```{r, fig.width = 5, fig.height = 5, fig.align = "center", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1,
  foreign = factor(1, levels = 1:2, labels = c("no", "yes")))

nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- factor(2, levels = 1:2, labels = c("no", "yes"))
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)

par(mar = c(4.5, 4, 0.5, 0.5))
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"))
```
The plot nicely depicts the nonlinear effect of variable `age` for the two levels
of `foreign`.

## Models for count data {#countreg}

This example is taken from @bamlss:Zeileis+Kleiber+Jackman:2008. The application is about
modeling the demand for medical care by elderly and was first analyzed by @bamlss:Deb+Trivedi:1997. The data can be downloaded on loaded into R with
```{r}
download.file(
  "https://www.jstatsoft.org/index.php/jss/article/downloadSuppFile/v027i08/DebTrivedi.rda.zip",
  "DebTrivedi.rda.zip"
)
unzip("DebTrivedi.rda.zip")
load("DebTrivedi.rda")
print(head(DebTrivedi))
```
The response variable in this data is the number of physician office visits, variable `ofp`. A
histogram of the count data response can be plotted with
```{r, eval=FALSE}
plot(table(DebTrivedi$ofp),
  xlab = "Number of physician office visits",
  ylab = "Frequency")
```
```{r, fig.width = 5, fig.height = 5, fig.align = "center", dev = "png", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
par(mar = c(4.5, 4, 0.5, 0.5))
plot(table(DebTrivedi$ofp),
  xlab = "Number of physician office visits",
  ylab = "Frequency")
```
which shows very large counts and also a large number of zero counts. Our first model is a
Poisson regression, which can be estimated with
```{r, echo=FALSE, message=FALSE, results='hide'}
if(!file.exists("figures/Poisson1.rda")) {
  f <- ofp ~ hosp + health + numchron + gender + school + privins
  set.seed(123)
  b1 <- bamlss(f, family = "poisson", data = DebTrivedi,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b1, file = "figures/Poisson1.rda")
} else {
  load("figures/Poisson1.rda")
}
```
```{r, eval=FALSE}
## Set the seed for reproducibly.
set.seed(123)

## Model formula.
f <- ofp ~ hosp + health + numchron + gender + school + privins

## Estimate model.
b1 <- bamlss(f, family = "poisson", data = DebTrivedi,
  n.iter = 12000, burnin = 2000, thin = 10)
```
The model summary is shown by
```{r}
summary(b1)
```
which shows "significant" effects for all covariates, since there are no zeros within
the 95% credible intervals. We can use randomized quantile residuals [@bamlss:Dunn+Gordon:1996]
to evaluate how good the models fits to the data.
```{r, eval = FALSE}
plot(b1, which = c("hist-resid", "qq-resid"))
```
```{r, fig.width = 8, fig.height = 4, fig.align = "center", echo = FALSE, dev = "png", echo=FALSE}
par(mfrow = c(1, 2))
plot(b1, which = c("hist-resid", "qq-resid"), spar = FALSE)
```
Clearly, the plots shows that the model performance is not very good at the tails of the data,
this might be caused by the large amount of zero counts in the data, which is not accounted for
using the Poisson distribution.

Therefore, another possible candidate for the response distribution is the
zero-inflated negative binomial distribution, which is implemented in the _gamlss_ package
[@bamlss:Stasinopoulos+Rigby:2018] in the function `ZINBI`. The family has 3 parameters `mu`,
`sigma` and `nu`. In this example we model distributional parameters `mu` and `sigma` in terms of
covariates by setting up the model formula with
```{r}
f <- list(
  ofp ~ hosp + health + numchron + gender + school + privins,
  sigma ~ hosp + health + numchron + gender + school + privins,
  nu ~ 1
)
```
Note, for parameter `nu` an intercept only model would also be the default if the specification
is omitted in the model formula. The model is estimated with (note, this can take some time)
```{r, echo=FALSE, message=FALSE, results='hide'}
library("gamlss")
if(!file.exists("figures/Poisson2.rda")) {
  set.seed(123)
  b2 <- bamlss(f, family = ZINBI, data = DebTrivedi,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b2, file = "figures/Poisson2.rda")
} else {
  load("figures/Poisson2.rda")
}
```
```{r, eval=FALSE}
## Get the gamlss families.
library("gamlss")

## Set the seed for reproducibly.
set.seed(123)

## Estimate model.
b2 <- bamlss(f, family = ZINBI, data = DebTrivedi,
  n.iter = 12000, burnin = 2000, thin = 10)
```
The model summary gives
```{r}
summary(b2)
```
and the corresponding randomized quantile residuals plots are created with
```{r, eval = FALSE}
plot(b2, which = c("hist-resid", "qq-resid"))
```
```{r, fig.width = 8, fig.height = 4, fig.align = "center", echo = FALSE, dev = "png"}
par(mfrow = c(1, 2))
plot(b2, which = c("hist-resid", "qq-resid"), spar = FALSE)
```
The plots indicate a better model fit compared to the Poisson model, however, for very large counts
the model performance could probably be improved further. The corresponding DIC values are 
```{r}
DIC(b1, b2)
```
which also state that the zero-inflated negative binomial model is better than the Poisson model.

## Large data sets

The R package _bamlss_ provides infrastructures to estimate models using large data sets. The default
algorithms for posterior mode estimation (see function `bfit()`) and MCMC simulation
(function `GMCMC()`) can be utilized in these situations. More precisely, the algorithms make advantage
of the fact that usually the number of unique observations is much smaller than the number of
total observations in the data (for a detailed description of the algorithms see
@bamlss:Lang+Umlauf+Wechselberger+Harttgen+Kneib:2014). For illustration, we load the precipitation
data of the HOMSTART-project which was first analyzed in @bamlss:Umlauf+Mayr+Messner:2012
(note, an internet connection is required).
```{r, eval=FALSE}
homstart_data(load = TRUE)
```
```{r, echo=FALSE, results='hide', message=FALSE}
if(!file.exists("figures/homstart.rda")) {
  owd <- getwd()
  setwd("figures")
  homstart_data(load = TRUE)
  save(homstart, file = "figures/homstart.rda")
  setwd(owd)
} else {
  load("figures/homstart.rda")
}
```
```{r}
print(head(homstart))
print(dim(homstart))
```
The data consists of 1063610 observations. To illustrate how large models can be fitted using
_bamlss_, we estimate a logit model using the binary response variable `bin`, which indicates if
it rained or not.
```{r, eval=FALSE}
## Set the seed for reproducibly.
set.seed(123)

## Model formula including a seasonal effect of the day
## of the year using harmonic regression.
f <- bin ~ weekend + elevation + cos1 + cos2 + sin1 + sin2

## Estimate model using only unique observations by setting
## argument binning = TRUE in the bamlss() call. Moreover,
## only use minimum storage for the returned object by
## setting light = TRUE.
b <- bamlss(f, data = homstart, family = "binomial",
  binning = TRUE, light = TRUE)
```
```{r, echo=FALSE, results='hide', message=FALSE}
if(!file.exists("figures/homstart_model.rda")) {
  set.seed(123)
  f <- bin ~ weekend + elevation + cos1 + cos2 + sin1 + sin2
  b <- bamlss(f, data = homstart, family = "binomial",
    binning = TRUE, light = TRUE)
  save(b, file = "figures/homstart_model.rda")
} else {
  load("figures/homstart_model.rda")
}
```
By setting `binning = TRUE`, the algorithms only use 33580 unique observations for model fitting.
This could in principle even be further utilized if each covariate represents one model term,
e.g.:
```{r}
f <- bin ~ weekend +
  s(elevation,bs="re",fx=TRUE) +
  s(cos1,bs="re",fx=TRUE) + s(cos2,bs="re",fx=TRUE) +
  s(sin1,bs="re",fx=TRUE) + s(sin2,bs="re",fx=TRUE)
```
Which is a bit of a hack using the random effects smooth constructor and setting `fx = TRUE`,
forcing the smoother to use fixed effects (linear effects) estimation. However, by doing so, the
resulting number of unique observations for the harmonic part is only 365 and for elevation only
46, which saves quite a lot of memory storage.

In this example, we are interested if it is more likely to rain on weekends than during
the week (see also @bamlss:Umlauf+Mayr+Messner:2012 and @bamlss:Umlauf+Klein+Zeileis:2017 for a
more detailed analysis). The model summary suggests
```{r}
summary(b)
```
that the weekend effect is very little and since we did not account for any spatial correlations
in this example, the effect is most probably not existing (at least in Austria). Note, because
we set `light = TRUE` in the `bamlss()` call, there will be no real sampler summary except of
the presented runtime of the sampler. The reason is, that computing the DIC from the posterior
samples can take quite long and can lead to memory storage problems, too.

## References

